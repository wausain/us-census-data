{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using python 3.10.5\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from scipy.stats import skew\n",
    "# from scipy.stats import kurtosis\n",
    "# from scipy.stats import chi2_contingency\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the data\n",
    "# there are no headers in the file so they will be imported separately and then appended to the train/test data\n",
    "# get columns before importing data so we can assign the column headers at import, you can either extract from the txt file or copy it over\n",
    "# we need to remove the columns containing '| instance weight' according to the instructions and add a column header for the target variable\n",
    "\n",
    "path_train = r\"C:\\Users\\wausa\\Downloads\\drive-download-20250217T195304Z-001\\census_income_learn.csv\"\n",
    "\n",
    "headers = ['age', 'class of worker', 'detailed industry recode', 'detailed occupation recode', 'education', 'wage per hour', 'enroll in edu inst last wk', 'marital stat',\n",
    "           'major industry code', 'major occupation code', 'race', 'hispanic origin', 'sex', 'member of a labor union', 'reason for unemployment',\n",
    "           'full or part time employment stat', 'capital gains', 'capital losses', 'dividends from stocks', 'tax filer stat', 'region of previous residence',\n",
    "           'state of previous residence', 'detailed household and family stat', 'detailed household summary in household', '| instance weight', 'instance weight',\n",
    "           'migration code-change in msa', 'migration code-change in reg', 'migration code-move within reg', 'live in this house 1 year ago', 'migration prev res in sunbelt',\n",
    "           'num persons worked for employer', 'family members under 18', 'country of birth father', 'country of birth mother', 'country of birth self', 'citizenship',\n",
    "           'own business or self employed', \"fill inc questionnaire for veteran's admin\", 'veterans benefits', 'weeks worked in year', 'year']\n",
    "headers.remove('| instance weight')\n",
    "headers.append('target') # the final column is the target variable\n",
    "\n",
    "continuous_data = ['age', 'wage per hour', 'capital gains', 'capital losses', 'dividends from stocks', 'num persons worked for employer', 'weeks worked in year']\n",
    "\n",
    "df = pd.read_csv(path_train, names=headers)\n",
    "df.shape\n",
    "\n",
    "# check the number of records and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>detailed industry recode</th>\n",
       "      <th>wage per hour</th>\n",
       "      <th>capital gains</th>\n",
       "      <th>capital losses</th>\n",
       "      <th>dividends from stocks</th>\n",
       "      <th>instance weight</th>\n",
       "      <th>num persons worked for employer</th>\n",
       "      <th>own business or self employed</th>\n",
       "      <th>veterans benefits</th>\n",
       "      <th>weeks worked in year</th>\n",
       "      <th>year</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "      <td>199523.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34.494199</td>\n",
       "      <td>15.352320</td>\n",
       "      <td>-0.134446</td>\n",
       "      <td>-0.060947</td>\n",
       "      <td>-0.104827</td>\n",
       "      <td>-0.060261</td>\n",
       "      <td>1740.380269</td>\n",
       "      <td>0.755334</td>\n",
       "      <td>0.175438</td>\n",
       "      <td>1.514833</td>\n",
       "      <td>23.174897</td>\n",
       "      <td>94.499672</td>\n",
       "      <td>0.062058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22.310895</td>\n",
       "      <td>18.067129</td>\n",
       "      <td>0.382961</td>\n",
       "      <td>0.225632</td>\n",
       "      <td>0.304645</td>\n",
       "      <td>0.231729</td>\n",
       "      <td>993.768156</td>\n",
       "      <td>0.807528</td>\n",
       "      <td>0.553694</td>\n",
       "      <td>0.851473</td>\n",
       "      <td>24.411488</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.241261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.225182</td>\n",
       "      <td>-0.097108</td>\n",
       "      <td>-0.147614</td>\n",
       "      <td>-0.104864</td>\n",
       "      <td>37.870000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.225182</td>\n",
       "      <td>-0.097108</td>\n",
       "      <td>-0.147614</td>\n",
       "      <td>-0.104864</td>\n",
       "      <td>1061.615000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.225182</td>\n",
       "      <td>-0.097108</td>\n",
       "      <td>-0.147614</td>\n",
       "      <td>-0.104864</td>\n",
       "      <td>1618.310000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>-0.225182</td>\n",
       "      <td>-0.097108</td>\n",
       "      <td>-0.147614</td>\n",
       "      <td>-0.104864</td>\n",
       "      <td>2188.610000</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>3.615560</td>\n",
       "      <td>3.099870</td>\n",
       "      <td>2.879785</td>\n",
       "      <td>3.937674</td>\n",
       "      <td>18656.300000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age  detailed industry recode  wage per hour  capital gains  \\\n",
       "count  199523.000000             199523.000000  199523.000000  199523.000000   \n",
       "mean       34.494199                 15.352320      -0.134446      -0.060947   \n",
       "std        22.310895                 18.067129       0.382961       0.225632   \n",
       "min         0.000000                  0.000000      -0.225182      -0.097108   \n",
       "25%        15.000000                  0.000000      -0.225182      -0.097108   \n",
       "50%        33.000000                  0.000000      -0.225182      -0.097108   \n",
       "75%        50.000000                 33.000000      -0.225182      -0.097108   \n",
       "max        90.000000                 51.000000       3.615560       3.099870   \n",
       "\n",
       "       capital losses  dividends from stocks  instance weight  \\\n",
       "count   199523.000000          199523.000000    199523.000000   \n",
       "mean        -0.104827              -0.060261      1740.380269   \n",
       "std          0.304645               0.231729       993.768156   \n",
       "min         -0.147614              -0.104864        37.870000   \n",
       "25%         -0.147614              -0.104864      1061.615000   \n",
       "50%         -0.147614              -0.104864      1618.310000   \n",
       "75%         -0.147614              -0.104864      2188.610000   \n",
       "max          2.879785               3.937674     18656.300000   \n",
       "\n",
       "       num persons worked for employer  own business or self employed  \\\n",
       "count                    199523.000000                  199523.000000   \n",
       "mean                          0.755334                       0.175438   \n",
       "std                           0.807528                       0.553694   \n",
       "min                           0.000000                       0.000000   \n",
       "25%                           0.000000                       0.000000   \n",
       "50%                           0.693147                       0.000000   \n",
       "75%                           1.609438                       0.000000   \n",
       "max                           1.945910                       2.000000   \n",
       "\n",
       "       veterans benefits  weeks worked in year           year         target  \n",
       "count      199523.000000         199523.000000  199523.000000  199523.000000  \n",
       "mean            1.514833             23.174897      94.499672       0.062058  \n",
       "std             0.851473             24.411488       0.500001       0.241261  \n",
       "min             0.000000              0.000000      94.000000       0.000000  \n",
       "25%             2.000000              0.000000      94.000000       0.000000  \n",
       "50%             2.000000              8.000000      94.000000       0.000000  \n",
       "75%             2.000000             52.000000      95.000000       0.000000  \n",
       "max             2.000000             52.000000      95.000000       1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at the continuous data, to determine figures asssoicated to each feature\n",
    "df.describe()\n",
    "\n",
    "# number of missing datapoints for each feature \n",
    "df.isnull().sum().value_counts() # shown as zero but looking at the data we have a number of missing records marked as '?' or so we \n",
    "# will replace the '?' values with NaN. It is also helpful to make the values NaN for encoding later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('?', np.nan) # we can see now that the empty values are now accounted for\n",
    "\n",
    "# data is full of whitespaces so they will be removed\n",
    "df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target column has been split into two columns (value +50,000 or -50,000), \n",
    "# we can avoid this by just converting the value to 1 or 0 in the column. The column is a string so we can just use the string value as the condition\n",
    "\n",
    "df['target'] = df['target'].apply(lambda x: 0 if x == '- 50000.' else 1)\n",
    "df['target'].value_counts()\n",
    "\n",
    "# imbalance in the target classes so we need to consider using some alternative method of evaluating the results aside from the \n",
    "# standard confusion matrix, something like an ROC curve\n",
    "\n",
    "# it may be pertinent to separate the data into numerical and categorical variables to determine the correlations between variables as \n",
    "# from the metadata we can see what variables are continues and which are nominal (outlined at the top of the file)\n",
    "\n",
    "df_continuous = df[continuous_data] # 7 continuous features\n",
    "df_categorical = df.drop(continuous_data, axis=1) # 35 categorical features\n",
    "df_categorical = df_categorical.astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 35 categorical features, which is high and will get higher if we use one hot encoding for example when we need to process the data\n",
    "# we can visualise the data\n",
    "# We have a large amount of zero values, \n",
    "# but those zeroes are useful records and indicative of actual values such as when 'wage per hour' is zero, it \n",
    "# is due to uneployment\n",
    "\n",
    "fig, ax = plt.subplots(5, 5)\n",
    "ax = ax.flatten()         # Convert axes to 1d array of length 9\n",
    "fig.set_size_inches(15, 25)\n",
    "\n",
    "for ax, col in zip(ax, df_categorical.columns):\n",
    "  sns.histplot(df_categorical[col], ax = ax,)\n",
    "  ax.tick_params(axis='x', labelrotation=45, labelsize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's a lot of categorical variables, and we can use \n",
    "# correlation to determine if we can reduce the feature count\n",
    "\n",
    "# we can view this correlation using Cramers V test, which is used to \n",
    "# calcualte the correlation between categorical values, in this \n",
    "# example, those which are nominal\n",
    "\n",
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - (k-1)*(r-1)/(n-1))\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    return np.sqrt((phi2corr / min( (kcorr-1), (rcorr-1))))\n",
    "\n",
    "# one example of categorical variable correlation\n",
    "contingency = pd.crosstab(df_categorical['region of previous residence'], df_categorical['state of previous residence']).values\n",
    "cramers = cramers_v(contingency)\n",
    "print(f'Cramer\\'s V correlation for \\'region of previous residence\\' and \\'state of previous residence\\' is {cramers}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the correlation for the categorical features using Cramer's V\n",
    "\n",
    "df_categorical_first15 = df_categorical.iloc[:, 0:15] # first 15 features, not enough memory to compare them all visually\n",
    "\n",
    "rows= []\n",
    "for var1 in df_categorical_first15:\n",
    "    col = []\n",
    "    for var2 in df_categorical_first15 :\n",
    "        contingency = pd.crosstab(df_categorical_first15[var1], df_categorical_first15[var2]).values\n",
    "        cramers = cramers_v(contingency)\n",
    "        col.append(round(cramers,2))\n",
    "    rows.append(col)\n",
    "\n",
    "cramers_results = np.array(rows)\n",
    "cramers_df = pd.DataFrame(cramers_results, columns = df_categorical_first15.columns, index =df_categorical_first15.columns)\n",
    "\n",
    "sns.heatmap(cramers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can drop the highly correlated fields to reduce the number of features\n",
    "# i.e., detailed industry recode and detailed occupation recode, major industry code and major occupation code, \n",
    "# detailed household and family stat and detailed household summary in household\n",
    "# migration code-change in msa, migration code-change in reg, migration code-move within reg, migration prev res in sunbelt\n",
    "# fill inc questionnaire for veteran\\'s admin and veterans benefits\n",
    "\n",
    "corr_cols_to_remove = ['detailed occupation recode', 'major occupation code', 'detailed household summary in household', 'migration code-change in msa', \n",
    "                     'migration code-change in reg', 'migration code-move within reg', 'migration prev res in sunbelt', 'fill inc questionnaire for veteran\\'s admin', \n",
    "                     'region of previous residence']\n",
    "\n",
    "df = df.drop(corr_cols_to_remove, axis=1) # remove columns from original df\n",
    "\n",
    "df_categorical = df_categorical.drop(corr_cols_to_remove, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numerical values we can immediately view the correlation as a heatmap\n",
    "\n",
    "sns.heatmap(df_continuous.corr(), annot=True, fmt='.2f')\n",
    "\n",
    "# we can see the numerical values are not necessarily correlated aside from 'number of persons worked for employer'/'weeks worked' \n",
    "# which has a fairly strong correlation at 0.75. We will keep all columns as the correlation is not that extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may consider scaling the continuous values if the range is high and we can also transform the same data if there's a high skew >0.5 or <-0.5\n",
    "\n",
    "# we can view the caegorical values\n",
    "fig, ax = plt.subplots(4, 2)\n",
    "ax = ax.flatten()         # Convert axes to 1d array of length 9\n",
    "fig.set_size_inches(15, 25)\n",
    "\n",
    "for ax, col in zip(ax, df_continuous.columns):\n",
    "  sns.histplot(df_continuous[col], ax = ax,)\n",
    "  ax.tick_params(axis='x', labelrotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see a lot of zero values here which provides a large amount of skew for this data, however given the nature of the data \n",
    "# it doesn't necessarily make sense to remove the data, rather it provides information about people who may be unemployed or have no investments\n",
    "\n",
    "# for example, most of those not working are either young (<20) or old/retired (> 60) as shown below\n",
    "df.loc[df['weeks worked in year'] == 0 , 'age'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can still look at the skew and kurotsis (removed due to length of output) of these continuous variables\n",
    "# skew above 0.5 or below -0.5 is generally significant and for Kurtosis values above 2 or below -2 \n",
    "\n",
    "for x in df_continuous.columns:\n",
    "    print(f'\\nThe skew of the variable {x} is: {skew(df[x])}')\n",
    "    # print(f'The kurtosis of the variable {x} is: {kurtosis(df[x])}')\n",
    "\n",
    "# we may consider applying a log transformation to reduce the skew.\n",
    "# we may also want to scale 'wage per hour', 'capital gains', 'capital losses', 'dividends from stocks' as they have large value ranges\n",
    "\n",
    "scaler = StandardScaler() # scale specific columns -> skew is unchanged\n",
    "\n",
    "cols_to_transform = ['wage per hour', 'capital gains', 'capital losses', 'dividends from stocks']\n",
    "for col in cols_to_transform:\n",
    "    df[col] = scaler.fit_transform(df[[col]])\n",
    "\n",
    "print(f'\\nAfter transforming: {cols_to_transform}')\n",
    "# now log transform the highly skewed columns 'wage per hour', 'capital gains', 'capital losses', 'dividends from stocks', 'num persons worked for employer'\n",
    "for col in ['wage per hour', 'capital gains', 'capital losses', 'dividends from stocks', 'num persons worked for employer']:\n",
    "    df[col] = np.log(df[[col]] + 1) # we need to add the 1 to the values as we cannot log transform the 0 values\n",
    "    # df[col] = np.sqrt(df[[col]] + 1) # culd sqrt transform instead, again, we need to add the 1 to the values as we cannot sqrt transform the 0 values\n",
    "\n",
    "# just looking at skew, skews are still large due to the large number of zeroes, but they are reduced\n",
    "for col in cols_to_transform:\n",
    "    print(f'\\nThe transformed skew of the variable {col} is: {skew(df[col])}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode data for the categorical values \n",
    "\n",
    "ohe_df = pd.get_dummies(df.drop('target', axis=1), dtype=np.uint8)\n",
    "target_df = df[['target']]\n",
    "\n",
    "# one hot encoding produces 300+ features which is a lot of features, \n",
    "# however we can still use sklearn for this, as a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a class imbalance for the target variable \n",
    "target_df.value_counts()\n",
    "\n",
    "# so we can choose to over or undersample the target. \n",
    "# However, this would mean we would lose a lot of data due to the \n",
    "# significance of the imbalance We can adjust the class weights in \n",
    "# the loss function instead, that way, we can keep all the data\n",
    "\n",
    "# additionally, during evaluation, by using metrics that are robust to class \n",
    "# imbalance, such as ROC and AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression with an L1 penalty, which is the Lasso regrssion, which \n",
    "# can shirink the coefficient of 'unimportant' features to 0 \n",
    "\n",
    "# whilst the continuous features have been transformed, it may be better to \n",
    "# transform them to closer represent a normal distributions, bu using a quantile \n",
    "# transformation for example, but we have a lot of useful zero data \n",
    "\n",
    "# play around with the strength of the L1 penalty\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', verbose=2)\n",
    "cv = cross_validate(model, ohe_df, target_df['target'], scoring='roc_auc', cv=5, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluated using ROC and AUC with a 5 fold cross validation \n",
    "# (again due to the class imbalance) \n",
    "cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# presenting the data\n",
    "\n",
    "log_odds = np.exp(cv['estimator'][0].coef_)[0]\n",
    "sort_idx = np.argsort(log_odds)\n",
    "\n",
    "dict(zip([ohe_df.columns[idx] for idx in sort_idx], log_odds[sort_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can evaluate the model by looking at the the regression coefficients for various variables\n",
    "\n",
    "# some of the most important featues for those making more than $50k p.a., include:\n",
    "# 'education_Doctorate degree(PhD EdD)',\n",
    "# 'education_Prof school degree (MD DDS DVM LLB JD)'\n",
    "# whilst some features that result in a reduction in the probability of making more than $50k p.a., include:\n",
    "# 'education_Less than 1st grade'\n",
    "# 'class of worker_Without pay'\n",
    "\n",
    "\n",
    "# Because we have used OHE, the figures represent the change in odds \n",
    "# associated with the specific feature, rather than resulting in a unit change\n",
    "# in probability as would be expected from a continuous variable\n",
    "\n",
    "# things to try if given more time\n",
    "# reduce overall dimensionality of the data, i.e., using PCA, but without losing too much information\n",
    "# Scaling/transforming categorical data\n",
    "# investigate and remove additional features by finding correaltion between continuous and categorical values\n",
    "# deep learning if willing to invest in more data/time"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
